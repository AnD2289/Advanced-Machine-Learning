{"cells":[{"cell_type":"markdown","metadata":{"id":"McmX1lPOfT4A"},"source":["# Programming Assignment 2: Sentiment analysis with SVM"]},{"cell_type":"markdown","metadata":{"id":"hiVJAzIdfT4D"},"source":["In this programming assignment, we will revisit the problem of sensiment analysis, but using a different approach. Recall that the task is to predict the *sentiment* (positive or negative) of a single sentence taken from a review of a movie, restaurant, or product. The data set consists of 3000 labeled sentences, which we divide into a training set of size 2500 and a test set of size 500. Previously we found a logistic regression classifier. Today we will use a support vector machine.\n","\n","Make sure the notebook is in the same folder that contains `full_set.txt`."]},{"cell_type":"markdown","metadata":{"id":"7ZbY__6mfT4E"},"source":["## 1. Load and preprocess data"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"76aeHbnLfT4F"},"outputs":[],"source":["%matplotlib inline\n","import string\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import time\n","import random\n","from progressbar import ProgressBar\n","matplotlib.rc('xtick', labelsize=14) \n","matplotlib.rc('ytick', labelsize=14)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4588,"status":"ok","timestamp":1644725964597,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"},"user_tz":300},"id":"b6IXEad4ivrl","outputId":"f9a1c840-d730-4dc4-ed0d-049536132d5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_A4m06K7iwuY"},"outputs":[],"source":["#Append the directory to your python path using sys\n","\n","import sys\n","prefix = '/content/gdrive/My Drive/'\n","# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\n","#customized_path_to_your_homework = 'ECE 4424  CS 4824/ProgAs1/ProgAs1_notebook_data'\n","customized_path_to_your_homework = 'ECE 4424  CS 4824/ProgAs2'\n","sys_path = prefix + customized_path_to_your_homework\n","sys.path.append(sys_path)"]},{"cell_type":"markdown","metadata":{"id":"794CG4n4fT4G"},"source":["The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). To be consistent with our notation from lecture, we will change the negative review label to '-1'."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"5gCkN7WAfT4G"},"outputs":[],"source":["## Read in the data set.\n","with open(\"/content/gdrive/My Drive/ECE 4424  CS 4824/ProgAs2/full_set.txt\") as f:\n","    content = f.readlines()\n","    \n","## Remove leading and trailing white space\n","content = [x.strip() for x in content]\n","\n","## Separate the sentences from the labels\n","sentences = [x.split(\"\\t\")[0] for x in content]\n","labels = [x.split(\"\\t\")[1] for x in content]\n","\n","## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n","y = np.array(labels, dtype='int8')\n","y = 2*y - 1"]},{"cell_type":"markdown","metadata":{"id":"aEGu1KSWfT4G"},"source":["### Preprocessing the text data\n","\n","To transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n","\n","1. Remove punctuation and numbers.\n","2. Transform all words to lower-case.\n","3. Remove _stop words_.\n","4. Convert the sentences into vectors, using a bag-of-words representation.\n","\n","We begin with first two steps."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"VtgYjTVIfT4H"},"outputs":[],"source":["## full_remove takes a string x and a list of characters removal_list \n","## returns x with all the characters in removal_list replaced by ' '\n","def full_remove(x, removal_list):\n","    for w in removal_list:\n","        x = x.replace(w, ' ')\n","    return x\n","\n","## Remove digits\n","digits = [str(x) for x in range(10)]\n","digit_less = [full_remove(x, digits) for x in sentences]\n","\n","## Remove punctuation\n","punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n","\n","## Make everything lower-case\n","sents_lower = [x.lower() for x in punc_less]"]},{"cell_type":"markdown","metadata":{"id":"-IEG7ouWfT4I"},"source":["### Stop words\n","\n","Stop words are words that are filtered out because they are believed to contain no useful information for the task at hand. These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they', and prepositions such 'to' and 'from'. We have put together a very small list of stop words, but these are by no means comprehensive. Feel free to use something different; for instance, larger lists can easily be found on the web."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"zfg5ivwtfT4J"},"outputs":[],"source":["## Define our stop words\n","stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n","\n","## Remove stop words\n","sents_split = [x.split() for x in sents_lower]\n","sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]"]},{"cell_type":"markdown","metadata":{"id":"AhVUWH4nfT4J"},"source":["### Bag of words\n","\n","In order to use linear classifiers on our data set, we need to transform our textual data into numeric data. The classical way to do this is known as the _bag of words_ representation. In this representation, each word is thought of as corresponding to a number in `{1, 2, ..., d}` where `d` is the size of our vocabulary. And each sentence is represented as a d-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.\n","\n","To do this transformation, we will make use of the `CountVectorizer` class in `scikit-learn` (Note that this is the only time you can call an external function from scikit-learn). We will cap the number of features at 4500, meaning a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful.\n","\n","Once we get the bag-of-words representation, append a '1' to the beginning of each vector to allow our linear classifier to learn a bias term."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":360,"status":"ok","timestamp":1644725978546,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"},"user_tz":300},"id":"Kxu8h4VhfT4J","outputId":"4df0f2e3-f3eb-46a1-fc93-a865cc9c1c7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["The original size:  (3000, 4500)\n","(3000, 4500)\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","## Transform to bag of words representation.\n","vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n","data_features = vectorizer.fit_transform(sents_processed)\n","data_mat = data_features.toarray()\n","print ('The original size: ',data_features.shape)\n","print(data_mat.shape)"]},{"cell_type":"markdown","metadata":{"id":"E8GEF77wfT4K"},"source":["### Training / test split\n","\n","Finally, we split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203,"status":"ok","timestamp":1644725980411,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"},"user_tz":300},"id":"UNdOtkTofT4L","outputId":"d8fbc675-b0b8-4597-8e1f-1e1cb2a8119e"},"outputs":[{"output_type":"stream","name":"stdout","text":["train data:  (2500, 4500)\n","train label:  (2500,)\n","test data:  (500, 4500)\n"]}],"source":["## Split the data into testing and training sets\n","np.random.seed(0)\n","test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n","train_inds = list(set(range(len(labels))) - set(test_inds))\n","\n","train_data = data_mat[train_inds,]\n","train_labels = y[train_inds]\n","\n","test_data = data_mat[test_inds,]\n","test_labels = y[test_inds]\n","\n","print(\"train data: \", train_data.shape)\n","print(\"train label: \", train_labels.shape)\n","print(\"test data: \", test_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"aA-1h79BfT4L"},"source":["## 2. Solving for soft-margin SVM\n","\n","\n","Recall that support vector machine (SVM) finds a linear decision boundary with the largest margin for a binary classification problem. Suppose we have a training dataset $\\{(x_{1},y_1),...,(x_n,y_n)\\}$\n","where $x_{i} \\in \\mathbb{R}^{d}$ are feature vectors and $y_i\\in\\{-1,+1\\}$ are labels.  The linear classifier is parametrized by $\\theta\\in \\mathbb{R}^{d}$ and $\\theta_0\\in\\mathbb{R}$, and predicts +1 at a point $x$ if $\\theta\\cdot x+\\theta_0>0$ and -1 otherwise. \n","\n","It turns out that the soft-margin SVM optimization is equivalent to the following unconstrained optimization:\n","$$\\underset{\\theta\\in\\mathbb{R}^d,\\theta\\in\\mathbb{R}}{\\text{min}}\\|\\theta\\|^2+C\\sum_{i=1}^n\\ell_{\\mathrm{hinge}}(y_i(\\theta\\cdot x_i+\\theta_0))$$\n","where  $\\ell_{\\mathrm{hinge}}(t)=\\max(0,1-t)$ is called the ``hinge loss,'' which takes value $1-t$ if $t<1$ and 0 otherwise. For example, $\\ell_{\\mathrm{hinge}}(-1)=2$, and $\\ell_{\\mathrm{hinge}}(2)=0$. \n"]},{"cell_type":"markdown","metadata":{"id":"NDsNyJuQfT4L"},"source":["It turns out that for gradient-based optimization, hinge loss may be difficult to deal with because it is not differentiable at point $t=1$. One solution is to use the ``smoothed version'' of hinge loss:\n","\n","$$\\ell_{\\mathrm{smooth-hinge}}(t) = \\begin{cases}\n","\\frac{1}{2} - t      & \\text{if} ~~ t \\le 0, \\\\\n","\\frac{1}{2} (1 - t)^2 & \\text{if} ~~ 0 < t < 1, \\\\\n","0                      & \\text{if} ~~ 1 \\le t\n","\\end{cases}$$\n","\n","\n","Thus, in the rest of the problem, we will consider the following optimization:\n","$$\\underset{\\theta\\in\\mathbb{R}^d,\\theta\\in\\mathbb{R}}{\\text{min}}\\|\\theta\\|^2+C\\sum_{i=1}^n\\ell_{\\mathrm{smooth-hinge}}(y_i(\\theta\\cdot x_i+\\theta_0))$$"]},{"cell_type":"markdown","metadata":{"id":"YcUXkY4ZfT4L"},"source":["**Task P2:** Implement the hinge loss function and the smooth hinge loss function. Plot the function $\\ell_{\\mathrm{hinge}}(t)$ and $\\ell_{\\mathrm{smooth-hinge}}(t)$for $t\\in[-5,5]$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_0Pa0YjfT4M","executionInfo":{"status":"ok","timestamp":1644725985798,"user_tz":300,"elapsed":159,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"}},"colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"95c20453-1c00-4ee1-adac-771c0d208027"},"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["def hinge_loss(t):\n","### STUDENT: Start of code ###\n","    a = np.array([0,1-t])\n","    loss_hinge = np.amax(a)\n","    return loss_hinge\n","### End of code ###\n","\n","\n","def hinge_loss_smooth(t):\n","### STUDENT: Start of code ###\n","    if t <= 0:\n","      loss_smooth_hinge = 0.5 - t\n","    elif t >0 and t <1:\n","      loss_smooth_hinge = 0.5*(1 - t)**2\n","    elif t>=1:\n","      loss_smooth_hinge = 0\n","    return loss_smooth_hinge\n","### End of code ###\n","\n","\n","#Plot\n","### STUDENT: Start of code ###\n","    \n","t = np.linspace(-5,5,100)\n","my_hinge = list()\n","my_smooth_hinge = list()\n","for u in t:\n","  my_val1 = hinge_loss(u)\n","  my_hinge.append(my_val1)\n","  my_val2 = hinge_loss_smooth(u)\n","  my_smooth_hinge.append(my_val2)\n","\n","plt.plot(t,my_hinge, linewidth=3)\n","plt.plot(t,my_smooth_hinge, linewidth=4)\n","plt.xlabel('t')\n","plt.ylabel('loss')\n","plt.grid()\n","#plt.show()\n","plt.legend([\"Hinge loss\", \"Smooth hinge loss\"])\n","plt.show()\n","#print(t)\n","\n","### End of code ###\n"]},{"cell_type":"markdown","metadata":{"id":"AMwgsBXwfT4M"},"source":["**Task P5:** Let $f(\\theta,\\theta_0)= \\|\\theta\\|^2+C\\sum_{i=1}^n\\ell_{\\mathrm{smooth-hinge}}(y_i(\\theta\\cdot x_i+\\theta_0))$ be the objective function of the optimization problem we want to solve. Implement the function that obtains the partial derivative $\\frac{\\partial }{\\partial \\theta}f(\\theta,\\theta_0)$ and $\\frac{\\partial }{\\partial \\theta_0}f(\\theta,\\theta_0)$. Also, print out the output of the code that calculates the derivatives at $\\theta=1$ and $\\theta_0=1$ with $C=1$.\n","\n","Hint: you need to calculate the partial derivative of the smoothed hinge loss for each data point separately, and add them together to obtain the result."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"YjVqPoJJfT4M"},"outputs":[],"source":["def weight_derivative(theta, theta0, C,feature_matrix, labels):\n","    # Input:\n","    # theta: weight vector theta, a numpy vector of dimension d\n","    # theta0: intercept theta0, a numpy vector of dimension 1\n","    # C: constant C\n","    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n","    # labels: true labels y, a numpy vector of dimension d, each with value -1 or +1\n","    # Output:\n","    # Derivative of the cost function with respect to the weight theta, grad_theta\n","    # Derivative of the cost function with respect to the weight theta0, grad_theta0\n","        \n","    ## STUDENT: Start of code ###\n","    sz = feature_matrix.shape\n","    grad_mx = np.zeros((sz[0],sz[1]))\n","    grad_mx0 = np.zeros((sz[0]))\n","    ss = theta0.shape\n","    \n","    for j in range(sz[0]):\n","      # if ss[0] > 1:\n","      t = labels[j]*(np.dot(theta,feature_matrix[j][:]) + theta0[0])\n","      if t <= 0:\n","        der_theta = ( (-1)*labels[j]*feature_matrix[j][:])\n","        der_theta0 = (-1)* labels[j]\n","        #print(\"if 1 iteration\", j)\n","      elif t >0 and t <1:\n","        der_theta = (-1)*labels[j]*feature_matrix[j][:]*(1-(labels[j]*(np.dot(theta,feature_matrix[j][:]) + theta0[0]))) #(-(1-t)*feature_matrix[j][:])\n","        der_theta0 = (-1)*labels[j]*(1-(labels[j]*(np.dot(theta,feature_matrix[j][:]) + theta0[0])))\n","        #print(\"if 2 iteration\", j)\n","      elif t>=1:\n","        der_theta = np.zeros(sz[1])\n","        der_theta0 = 0\n","        #print(\"if 3 iteration\", j)\n","      # elif bool(ss):\n","      #   t = labels[j]*(np.dot(theta,feature_matrix[j][:]) + theta0)\n","      #   if t <= 0:\n","      #     der_theta = ( - labels[j]*feature_matrix[j][:])\n","      #     der_theta0 = - labels[j]\n","      #   elif t >0 and t <1:\n","      #     der_theta = (-t*feature_matrix[j][:])\n","      #     der_theta0 = -(1-(labels[j]*(np.dot(theta,feature_matrix[j][:]) + theta0[j])))*labels[j]\n","      #   elif t>=1:\n","      #     der_theta = 0\n","      #     der_theta0 = 0\n","      grad_mx[j][:] = der_theta\n","      grad_mx0[j] = der_theta0\n","\n","    grad_theta = 2*theta + C*grad_mx.sum(axis=0)\n","    grad_theta0 =  C*grad_mx0.sum(axis=0)\n","   \n","    return grad_theta, grad_theta0\n","    # End of code ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190,"status":"ok","timestamp":1644725992916,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"},"user_tz":300},"id":"Zlwh7IlyfT4M","outputId":"9318acf4-1a01-49e9-eed3-5b2e197168c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 2.  3.  3.  4.  3.  3. 38.  5.  2.  2.]\n","1250.0\n"]}],"source":["# STUDENT: PRINT THE OUTPUT AND COPY IT TO THE SOLUTION FILE\n","theta = np.ones(data_mat.shape[1]) # a weight of all 1s\n","theta0 = np.ones(1) # a number 1\n","C = 1\n","grad_theta, grad_theta0 = weight_derivative(theta, theta0, C,train_data,train_labels)\n","\n","print (grad_theta[:10])\n","print (grad_theta0)\n"]},{"cell_type":"markdown","metadata":{"id":"cpIotiC0fT4N"},"source":["**Task P6:**  For sentiment analysis data, choose a value for the trade-off parameter $C$. Report the training error at convergence and the testing error. \n","\n","Note:  you can just use the same gradient descent algorithm that we wrote in assignment 2, or use the adam_optimizer provided below.\n","\n","Here is an [article](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c#:~:text=Adam%20%5B1%5D%20is%20an%20adaptive,for%20training%20deep%20neural%20networks.&text=The%20algorithms%20leverages%20the,learning%20rates%20for%20each%20parameter) on the Adam optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3ndjpAs3fT4N"},"outputs":[],"source":["def objective(feature_matrix, labels, theta,theta0, C):\n","    score = (feature_matrix.dot(theta)+theta0)*labels\n","    return np.sum(theta**2)+C*np.sum([hinge_loss_smooth(t) for t in score])"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"LaQzPDtEfT4N"},"outputs":[],"source":["def adam_optimizer(feature_matrix, labels, initial_theta,initial_theta0, C, step_size=0.01, tolerance=0.01, b1=0.9, b2=0.999, eps=10**-8):\n","    # Gradient descent algorithm for logistic regression problem    \n","    \n","    # Input:\n","    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n","    # labels: true labels y, a numpy vector of dimension d\n","    # initial_theta: initial theta to start with, a numpy vector of dimension d\n","    # initial_theta0: initial theta0 to start with, a numpy vector of dimension 1\n","    # step_size: step size of update\n","    # tolerance: tolerace epsilon for stopping condition\n","    # Parameters by Adam optimizer\n","    # Output:\n","    # Weights obtained after convergence\n","\n","    converged = False \n","    m = np.zeros(len(initial_theta))\n","    v = np.zeros(len(initial_theta))\n","    m0 = np.zeros(1)\n","    v0 = np.zeros(1)\n","    theta = np.array(initial_theta) # current iterate\n","    theta0 = np.array(initial_theta0) # current iterate\n","    i = 0\n","    j=0\n","    while not converged:\n","    #while j< 10000:\n","        # impelementation of what the gradient descent algorithm does in every iteration\n","        # Refer back to the update rule listed above: update the weight\n","        i += 1\n","        j=i\n","        grad_theta, grad_theta0  = weight_derivative(theta, theta0, C,feature_matrix, labels)\n","        \n","        m = (1 - b1) * grad_theta      + b1 * m  # First  moment estimate.\n","        v = (1 - b2) * (grad_theta**2) + b2 * v  # Second moment estimate.\n","        mhat = m / (1 - b1**(i + 1))    # Bias correction.\n","        vhat = v / (1 - b2**(i + 1))\n","        theta = theta - step_size*mhat/(np.sqrt(vhat) + eps)\n","        \n","        m0 = (1 - b1) * grad_theta0      + b1 * m0  # First  moment estimate.\n","        v0 = (1 - b2) * (grad_theta0**2) + b2 * v0  # Second moment estimate.\n","        mhat0 = m0 / (1 - b1**(i + 1))    # Bias correction.\n","        vhat0 = v0 / (1 - b2**(i + 1))\n","        theta0 = theta0 - step_size*mhat0/(np.sqrt(vhat0) + eps)\n","        \n","        # Compute the gradient magnitude:\n","        \n","        gradient_magnitude = np.sqrt(np.sum(grad_theta**2))\n","        \n","        # Check the stopping condition to decide whether you want to stop the iterations\n","        \n","        if gradient_magnitude < tolerance:\n","            converged = True\n","        \n","        preds_train = model_predict(train_data,theta,theta0)\n","\n","        ## Compute errors\n","        errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n","\n","        print (\"Iteration: \",i,\"objective: \",objective(feature_matrix, labels, theta,theta0, C),\"tr err: \",float(errs_train)/len(train_labels),\"gradient_magnitude: \", gradient_magnitude) # for us to check about convergence\n","        \n","    return(theta, theta0)\n","\n","### End of code ###"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"z6K0NwbbfT4N"},"outputs":[],"source":["def model_predict(feature_matrix,theta,theta0):\n","# Prediction made by SVM \n","    \n","    # Input:\n","    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n","    #                 note we have included the dummy feature as the first column of the feature_matrix\n","    # theta: weight theta, a numpy vector of dimension d\n","    # theta0: weight theta0, a numpy vector of dimension 1\n","    # Output:\n","    # labels: predicted labels, a numpy vector of dimension n\n","    \n","    h =  feature_matrix.dot(theta)+theta0\n","    y_h = (h >= 0)*2-1\n","    \n","    return y_h    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8xE02MjfT4O","outputId":"c6da4984-ad06-4cf8-b0af-15203f5015fb","executionInfo":{"status":"ok","timestamp":1644726461397,"user_tz":300,"elapsed":48948,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration:  1 objective:  5744.056138901959 tr err:  0.5 gradient_magnitude:  861.0433400306346\n","Iteration:  2 objective:  2603.860324555173 tr err:  0.474 gradient_magnitude:  853.4614786482572\n","Iteration:  3 objective:  2003.1389415438102 tr err:  0.4236 gradient_magnitude:  719.6627883714607\n","Iteration:  4 objective:  2757.1400803195693 tr err:  0.4268 gradient_magnitude:  714.4866332583961\n","Iteration:  5 objective:  2764.0214714500753 tr err:  0.3976 gradient_magnitude:  911.7033810979287\n","Iteration:  6 objective:  2223.8326375749402 tr err:  0.3096 gradient_magnitude:  907.1923931148376\n","Iteration:  7 objective:  1587.5785524393732 tr err:  0.1784 gradient_magnitude:  801.808856977141\n","Iteration:  8 objective:  1252.3047802101464 tr err:  0.088 gradient_magnitude:  499.5090209663632\n","Iteration:  9 objective:  1219.1653894564452 tr err:  0.0648 gradient_magnitude:  202.3249562156505\n","Iteration:  10 objective:  1335.961543854242 tr err:  0.1008 gradient_magnitude:  119.69773744861357\n","Iteration:  11 objective:  1478.1537908630437 tr err:  0.1464 gradient_magnitude:  249.15691200639338\n","Iteration:  12 objective:  1538.7728841054886 tr err:  0.1756 gradient_magnitude:  374.84443719563836\n","Iteration:  13 objective:  1450.4737521868865 tr err:  0.176 gradient_magnitude:  453.17661643518966\n","Iteration:  14 objective:  1227.8386333786084 tr err:  0.1332 gradient_magnitude:  471.90676380431336\n","Iteration:  15 objective:  984.4888080453484 tr err:  0.0852 gradient_magnitude:  407.16458445855966\n","Iteration:  16 objective:  844.7328542933993 tr err:  0.058 gradient_magnitude:  253.25747086042443\n","Iteration:  17 objective:  825.7477453380034 tr err:  0.0612 gradient_magnitude:  104.80237673653954\n","Iteration:  18 objective:  874.7666473092083 tr err:  0.0792 gradient_magnitude:  96.210567795385\n","Iteration:  19 objective:  926.3581140102139 tr err:  0.1008 gradient_magnitude:  189.96658219035265\n","Iteration:  20 objective:  911.7893738417256 tr err:  0.1104 gradient_magnitude:  275.2280959257853\n","Iteration:  21 objective:  810.0916934740719 tr err:  0.09 gradient_magnitude:  317.5134897316036\n","Iteration:  22 objective:  682.1846721596527 tr err:  0.062 gradient_magnitude:  287.2285075490354\n","Iteration:  23 objective:  621.4685615644369 tr err:  0.0488 gradient_magnitude:  167.77277468035055\n","Iteration:  24 objective:  639.7547246714132 tr err:  0.0692 gradient_magnitude:  78.25091976332943\n","Iteration:  25 objective:  665.2007970227188 tr err:  0.0804 gradient_magnitude:  188.62511003387473\n","Iteration:  26 objective:  640.7772741387926 tr err:  0.0748 gradient_magnitude:  262.3452308216723\n","Iteration:  27 objective:  573.8398637951086 tr err:  0.0436 gradient_magnitude:  265.5157714568111\n","Iteration:  28 objective:  523.3868550711343 tr err:  0.0344 gradient_magnitude:  179.25226609376597\n","Iteration:  29 objective:  513.965698030782 tr err:  0.0352 gradient_magnitude:  58.63669096013923\n","Iteration:  30 objective:  520.9609037239255 tr err:  0.0424 gradient_magnitude:  118.72186515888774\n","Iteration:  31 objective:  505.7959760698633 tr err:  0.0412 gradient_magnitude:  196.02584324133207\n","Iteration:  32 objective:  464.540974621686 tr err:  0.0312 gradient_magnitude:  207.17938164321617\n","Iteration:  33 objective:  431.63283763372584 tr err:  0.0224 gradient_magnitude:  137.2030306639346\n","Iteration:  34 objective:  425.15417697577107 tr err:  0.0244 gradient_magnitude:  45.66928652665456\n","Iteration:  35 objective:  425.4621226973579 tr err:  0.024 gradient_magnitude:  118.61439853659802\n","Iteration:  36 objective:  406.7213229633405 tr err:  0.0192 gradient_magnitude:  184.85497474165916\n","Iteration:  37 objective:  383.99889045929893 tr err:  0.018 gradient_magnitude:  162.7159767830709\n","Iteration:  38 objective:  380.5575964171492 tr err:  0.0204 gradient_magnitude:  62.59830919749411\n","Iteration:  39 objective:  380.7655279636729 tr err:  0.0232 gradient_magnitude:  89.36971964401563\n","Iteration:  40 objective:  367.29889501466965 tr err:  0.0204 gradient_magnitude:  140.15528128208595\n","Iteration:  41 objective:  355.24263411690106 tr err:  0.0168 gradient_magnitude:  100.91241334733394\n","Iteration:  42 objective:  353.6878272794613 tr err:  0.0156 gradient_magnitude:  35.56448320592193\n","Iteration:  43 objective:  349.2619661544499 tr err:  0.0152 gradient_magnitude:  91.96703758742535\n","Iteration:  44 objective:  338.4012629550116 tr err:  0.0152 gradient_magnitude:  104.33277241512054\n","Iteration:  45 objective:  332.71541350805194 tr err:  0.016 gradient_magnitude:  52.56698960294883\n","Iteration:  46 objective:  330.7028734538584 tr err:  0.018 gradient_magnitude:  45.71317268975297\n","Iteration:  47 objective:  325.6291425082493 tr err:  0.0168 gradient_magnitude:  78.69380863581142\n","Iteration:  48 objective:  320.9200907683893 tr err:  0.016 gradient_magnitude:  61.266360782813415\n","Iteration:  49 objective:  319.3377809840076 tr err:  0.0164 gradient_magnitude:  22.895365537334104\n","Iteration:  50 objective:  316.5851383879341 tr err:  0.0156 gradient_magnitude:  50.184403814277196\n","Iteration:  51 objective:  312.85570599309165 tr err:  0.0152 gradient_magnitude:  55.38658833833148\n","Iteration:  52 objective:  311.6124592650309 tr err:  0.0144 gradient_magnitude:  24.23682614960846\n","Iteration:  53 objective:  309.89282103398307 tr err:  0.014 gradient_magnitude:  41.4652303475412\n","Iteration:  54 objective:  307.30650980011217 tr err:  0.014 gradient_magnitude:  45.77193163078473\n","Iteration:  55 objective:  306.34837525383705 tr err:  0.0132 gradient_magnitude:  19.60228063907663\n","Iteration:  56 objective:  304.8997867900402 tr err:  0.0124 gradient_magnitude:  32.624186832377646\n","Iteration:  57 objective:  302.87441536216716 tr err:  0.0124 gradient_magnitude:  34.60735624941698\n","Iteration:  58 objective:  301.918436405159 tr err:  0.0128 gradient_magnitude:  15.403261485771502\n","Iteration:  59 objective:  301.0647533669962 tr err:  0.0124 gradient_magnitude:  26.402186178516\n","Iteration:  60 objective:  300.16074800481715 tr err:  0.0124 gradient_magnitude:  28.188998176369683\n","Iteration:  61 objective:  299.70093931100365 tr err:  0.0124 gradient_magnitude:  15.869571931980865\n","Iteration:  62 objective:  298.80154271567574 tr err:  0.0124 gradient_magnitude:  22.386821174726933\n","Iteration:  63 objective:  297.7450174520227 tr err:  0.012 gradient_magnitude:  21.928832515937327\n","Iteration:  64 objective:  297.156569349638 tr err:  0.012 gradient_magnitude:  14.68217988777953\n","Iteration:  65 objective:  296.52041272069613 tr err:  0.0116 gradient_magnitude:  20.930319391466472\n","Iteration:  66 objective:  295.9839193229799 tr err:  0.012 gradient_magnitude:  17.14384738931784\n","Iteration:  67 objective:  295.714828506656 tr err:  0.012 gradient_magnitude:  12.584963402469857\n","Iteration:  68 objective:  295.21235688033465 tr err:  0.0124 gradient_magnitude:  16.947861986288952\n","Iteration:  69 objective:  294.652723627137 tr err:  0.0124 gradient_magnitude:  11.798707569804275\n","Iteration:  70 objective:  294.17671101089826 tr err:  0.0116 gradient_magnitude:  10.86958013319754\n","Iteration:  71 objective:  293.7467094096009 tr err:  0.0112 gradient_magnitude:  13.578927450134728\n","Iteration:  72 objective:  293.4666138774446 tr err:  0.0116 gradient_magnitude:  9.388956050750712\n","Iteration:  73 objective:  293.157412665365 tr err:  0.0124 gradient_magnitude:  10.70186938808373\n","Iteration:  74 objective:  292.8089720054645 tr err:  0.0128 gradient_magnitude:  10.544191524305317\n","Iteration:  75 objective:  292.5851196607996 tr err:  0.0128 gradient_magnitude:  7.158574774741241\n","Iteration:  76 objective:  292.3716948763821 tr err:  0.0128 gradient_magnitude:  8.976776611886987\n","Iteration:  77 objective:  292.1648293470788 tr err:  0.0124 gradient_magnitude:  7.390234889160795\n","Iteration:  78 objective:  291.990418648745 tr err:  0.0124 gradient_magnitude:  6.708016469471178\n","Iteration:  79 objective:  291.8613398635516 tr err:  0.0124 gradient_magnitude:  7.931401250910915\n","Iteration:  80 objective:  291.7926093325029 tr err:  0.0124 gradient_magnitude:  5.392138883054796\n","Iteration:  81 objective:  291.69689266752835 tr err:  0.0124 gradient_magnitude:  6.472728730668365\n","Iteration:  82 objective:  291.58176063067305 tr err:  0.0124 gradient_magnitude:  6.134722791965899\n","Iteration:  83 objective:  291.5070941844608 tr err:  0.0124 gradient_magnitude:  4.911966082752777\n","Iteration:  84 objective:  291.425544447637 tr err:  0.0124 gradient_magnitude:  6.648238184901695\n","Iteration:  85 objective:  291.3629395583489 tr err:  0.012 gradient_magnitude:  4.832109059328709\n","Iteration:  86 objective:  291.3054809601789 tr err:  0.0116 gradient_magnitude:  4.554158416038201\n","Iteration:  87 objective:  291.23119001530665 tr err:  0.0116 gradient_magnitude:  5.359469267234401\n","Iteration:  88 objective:  291.1707114135671 tr err:  0.012 gradient_magnitude:  3.89802067101247\n","Iteration:  89 objective:  291.10392948625827 tr err:  0.012 gradient_magnitude:  5.199993281670485\n","Iteration:  90 objective:  291.0452314469168 tr err:  0.012 gradient_magnitude:  4.499054357679146\n","Iteration:  91 objective:  290.9980222889135 tr err:  0.0116 gradient_magnitude:  3.643268334635867\n","Iteration:  92 objective:  290.9448168831117 tr err:  0.012 gradient_magnitude:  3.9971855470960467\n","Iteration:  93 objective:  290.90295392104065 tr err:  0.0116 gradient_magnitude:  3.0848689385731394\n","Iteration:  94 objective:  290.8635354680517 tr err:  0.0112 gradient_magnitude:  3.480952394719837\n","Iteration:  95 objective:  290.83280496058035 tr err:  0.0112 gradient_magnitude:  2.551961456874257\n","Iteration:  96 objective:  290.8085667944871 tr err:  0.0112 gradient_magnitude:  2.915589337830034\n","Iteration:  97 objective:  290.78223094825836 tr err:  0.0112 gradient_magnitude:  2.923824503079547\n","Iteration:  98 objective:  290.7631652702156 tr err:  0.012 gradient_magnitude:  2.197553893730091\n","Iteration:  99 objective:  290.7392893101297 tr err:  0.0116 gradient_magnitude:  2.8199338514899726\n","Iteration:  100 objective:  290.72056817610417 tr err:  0.0116 gradient_magnitude:  1.9126765835324262\n","Iteration:  101 objective:  290.708583430243 tr err:  0.0116 gradient_magnitude:  2.3917480444948858\n","Iteration:  102 objective:  290.698172536439 tr err:  0.0124 gradient_magnitude:  2.029910021928453\n","Iteration:  103 objective:  290.68487584149545 tr err:  0.0124 gradient_magnitude:  1.9918724008576691\n","Iteration:  104 objective:  290.6719728194789 tr err:  0.0124 gradient_magnitude:  1.9900091794422043\n","Iteration:  105 objective:  290.6657827105091 tr err:  0.0124 gradient_magnitude:  1.7902051334836833\n","Iteration:  106 objective:  290.65864404160817 tr err:  0.0124 gradient_magnitude:  2.363752384799482\n","Iteration:  107 objective:  290.65051419519807 tr err:  0.0128 gradient_magnitude:  1.5703051390842129\n","Iteration:  108 objective:  290.6396308352835 tr err:  0.012 gradient_magnitude:  1.756849391166681\n","Iteration:  109 objective:  290.6302095711277 tr err:  0.012 gradient_magnitude:  1.5300030857306408\n","Iteration:  110 objective:  290.6247717185514 tr err:  0.012 gradient_magnitude:  1.3777828222445165\n","Iteration:  111 objective:  290.61962582276817 tr err:  0.012 gradient_magnitude:  1.5351012531528785\n","Iteration:  112 objective:  290.61302474963793 tr err:  0.0124 gradient_magnitude:  1.121177183896732\n","Iteration:  113 objective:  290.6062311692209 tr err:  0.0124 gradient_magnitude:  1.4192273334732868\n","Iteration:  114 objective:  290.60326758046034 tr err:  0.0116 gradient_magnitude:  0.951052716383018\n","Iteration:  115 objective:  290.60000377548846 tr err:  0.0116 gradient_magnitude:  1.352522779857619\n","Iteration:  116 objective:  290.5968265391747 tr err:  0.012 gradient_magnitude:  0.9332970030346452\n","Iteration:  117 objective:  290.5930173111962 tr err:  0.012 gradient_magnitude:  1.0230261730246681\n","Iteration:  118 objective:  290.58901487327137 tr err:  0.012 gradient_magnitude:  0.9182649476101982\n","Iteration:  119 objective:  290.58610048591004 tr err:  0.0124 gradient_magnitude:  0.8287459459706384\n","Iteration:  120 objective:  290.5844584452793 tr err:  0.0124 gradient_magnitude:  0.852435733227523\n","Iteration:  121 objective:  290.58360900528544 tr err:  0.0128 gradient_magnitude:  0.7268265390062338\n","Iteration:  122 objective:  290.58175919537285 tr err:  0.0124 gradient_magnitude:  0.9316901042024903\n","Iteration:  123 objective:  290.57961007093286 tr err:  0.012 gradient_magnitude:  0.6025291971602551\n","Iteration:  124 objective:  290.57777690112886 tr err:  0.012 gradient_magnitude:  0.8606262135847828\n","Iteration:  125 objective:  290.5771986065597 tr err:  0.012 gradient_magnitude:  0.5735487761207524\n","Iteration:  126 objective:  290.5761755664904 tr err:  0.012 gradient_magnitude:  0.7907743798278408\n","Iteration:  127 objective:  290.57490724974787 tr err:  0.012 gradient_magnitude:  0.6315904658189329\n","Iteration:  128 objective:  290.5739643616506 tr err:  0.012 gradient_magnitude:  0.518461729816821\n","Iteration:  129 objective:  290.5731666127836 tr err:  0.0124 gradient_magnitude:  0.47457287957239036\n","Iteration:  130 objective:  290.57192938880326 tr err:  0.0124 gradient_magnitude:  0.5111673144355948\n","Iteration:  131 objective:  290.5710223613569 tr err:  0.0124 gradient_magnitude:  0.4845381039921108\n","Iteration:  132 objective:  290.57054256928836 tr err:  0.0124 gradient_magnitude:  0.4192302422040323\n","Iteration:  133 objective:  290.56995905944063 tr err:  0.0124 gradient_magnitude:  0.4168096214542102\n","Iteration:  134 objective:  290.56945214919665 tr err:  0.012 gradient_magnitude:  0.40947304738922335\n","Iteration:  135 objective:  290.5689107668535 tr err:  0.012 gradient_magnitude:  0.46120358433218156\n","Iteration:  136 objective:  290.5683920691898 tr err:  0.012 gradient_magnitude:  0.3273738910446319\n","Iteration:  137 objective:  290.5678905744007 tr err:  0.012 gradient_magnitude:  0.37652265630766185\n","Iteration:  138 objective:  290.56764510386154 tr err:  0.012 gradient_magnitude:  0.3049604835637352\n","Iteration:  139 objective:  290.5673739346867 tr err:  0.012 gradient_magnitude:  0.36239989987928023\n","Iteration:  140 objective:  290.5670757239455 tr err:  0.012 gradient_magnitude:  0.2591307011189655\n","Iteration:  141 objective:  290.5667463425164 tr err:  0.012 gradient_magnitude:  0.34877015383385324\n","Iteration:  142 objective:  290.5665437116251 tr err:  0.012 gradient_magnitude:  0.24199466136747272\n","Iteration:  143 objective:  290.5662994667101 tr err:  0.012 gradient_magnitude:  0.29469496414324153\n","Iteration:  144 objective:  290.5660786057405 tr err:  0.012 gradient_magnitude:  0.21155333624369943\n","Iteration:  145 objective:  290.56591906184394 tr err:  0.012 gradient_magnitude:  0.27752246787794804\n","Iteration:  146 objective:  290.56578702386366 tr err:  0.012 gradient_magnitude:  0.2175524222013935\n","Iteration:  147 objective:  290.56562757465207 tr err:  0.012 gradient_magnitude:  0.21275134267720475\n","Iteration:  148 objective:  290.5655502457005 tr err:  0.012 gradient_magnitude:  0.16250348688786226\n","Iteration:  149 objective:  290.56545915720164 tr err:  0.012 gradient_magnitude:  0.2131279055745774\n","Iteration:  150 objective:  290.5653359701678 tr err:  0.012 gradient_magnitude:  0.17202154702630282\n","Iteration:  151 objective:  290.5652121315064 tr err:  0.012 gradient_magnitude:  0.19053142850002694\n","Iteration:  152 objective:  290.56512368516206 tr err:  0.012 gradient_magnitude:  0.15140234707042\n","Iteration:  153 objective:  290.5650623681817 tr err:  0.012 gradient_magnitude:  0.1742329465668533\n","Iteration:  154 objective:  290.5650113274928 tr err:  0.012 gradient_magnitude:  0.14253308539379567\n","Iteration:  155 objective:  290.5649428619037 tr err:  0.012 gradient_magnitude:  0.15967157496778503\n","Iteration:  156 objective:  290.5648862656501 tr err:  0.012 gradient_magnitude:  0.1378783336621533\n","Iteration:  157 objective:  290.56483648969316 tr err:  0.012 gradient_magnitude:  0.11980838409517067\n","Iteration:  158 objective:  290.5647794604969 tr err:  0.012 gradient_magnitude:  0.12351987903804762\n","Iteration:  159 objective:  290.56473676538735 tr err:  0.012 gradient_magnitude:  0.10722963128197995\n","Iteration:  160 objective:  290.56470657884694 tr err:  0.012 gradient_magnitude:  0.10468979630857075\n","Iteration:  161 objective:  290.56466858348097 tr err:  0.012 gradient_magnitude:  0.0939782259980259\n","Iteration:  162 objective:  290.56463163433517 tr err:  0.012 gradient_magnitude:  0.09249170818440898\n","Iteration:  163 objective:  290.5646016921728 tr err:  0.012 gradient_magnitude:  0.09117516069159487\n","Iteration:  164 objective:  290.5645845187224 tr err:  0.012 gradient_magnitude:  0.08251141143046846\n","Iteration:  165 objective:  290.5645635083125 tr err:  0.012 gradient_magnitude:  0.08170906160785162\n","Iteration:  166 objective:  290.5645411653819 tr err:  0.012 gradient_magnitude:  0.07503793599872642\n","Iteration:  167 objective:  290.56452657981106 tr err:  0.012 gradient_magnitude:  0.07807754804713528\n","Iteration:  168 objective:  290.564512770135 tr err:  0.012 gradient_magnitude:  0.06199578793751834\n","Iteration:  169 objective:  290.56449362743786 tr err:  0.012 gradient_magnitude:  0.08026181036360562\n","Iteration:  170 objective:  290.56448333802064 tr err:  0.012 gradient_magnitude:  0.06283331808250688\n","Iteration:  171 objective:  290.56447566244333 tr err:  0.012 gradient_magnitude:  0.05955325199981394\n","Iteration:  172 objective:  290.56446527575713 tr err:  0.012 gradient_magnitude:  0.053814579628118996\n","Iteration:  173 objective:  290.564452801999 tr err:  0.012 gradient_magnitude:  0.05387938766943593\n","Iteration:  174 objective:  290.56444178382253 tr err:  0.012 gradient_magnitude:  0.043797616513534275\n","Iteration:  175 objective:  290.5644340024247 tr err:  0.012 gradient_magnitude:  0.047771392815390515\n","Iteration:  176 objective:  290.5644282325501 tr err:  0.012 gradient_magnitude:  0.039144153116546405\n","Iteration:  177 objective:  290.5644184846168 tr err:  0.012 gradient_magnitude:  0.046023812632311215\n","Iteration:  178 objective:  290.5644119723696 tr err:  0.012 gradient_magnitude:  0.033293691266171155\n","Iteration:  179 objective:  290.5644094085261 tr err:  0.012 gradient_magnitude:  0.04121363767777748\n","Iteration:  180 objective:  290.564405361487 tr err:  0.012 gradient_magnitude:  0.035254484625439605\n","Iteration:  181 objective:  290.5644001533723 tr err:  0.012 gradient_magnitude:  0.03927663380532881\n","Iteration:  182 objective:  290.5643979129975 tr err:  0.012 gradient_magnitude:  0.0316329362416252\n","Iteration:  183 objective:  290.56439456031785 tr err:  0.012 gradient_magnitude:  0.037274117013081934\n","Iteration:  184 objective:  290.564391030364 tr err:  0.012 gradient_magnitude:  0.030108884980938548\n","Iteration:  185 objective:  290.5643887489097 tr err:  0.012 gradient_magnitude:  0.03916743354465205\n","Iteration:  186 objective:  290.5643863707353 tr err:  0.012 gradient_magnitude:  0.024330365202234753\n","Iteration:  187 objective:  290.5643832329175 tr err:  0.012 gradient_magnitude:  0.028332231742255052\n","Iteration:  188 objective:  290.56438149445273 tr err:  0.012 gradient_magnitude:  0.022268068370277266\n","Iteration:  189 objective:  290.56437991834275 tr err:  0.012 gradient_magnitude:  0.023671334859101067\n","Iteration:  190 objective:  290.56437773605376 tr err:  0.012 gradient_magnitude:  0.021657844655721764\n","Iteration:  191 objective:  290.5643760414714 tr err:  0.012 gradient_magnitude:  0.01873139989780867\n","Iteration:  192 objective:  290.56437492617056 tr err:  0.012 gradient_magnitude:  0.02244954433913137\n","Iteration:  193 objective:  290.56437406032114 tr err:  0.012 gradient_magnitude:  0.017658040399839692\n","Iteration:  194 objective:  290.56437296009994 tr err:  0.012 gradient_magnitude:  0.019017088002610167\n","Iteration:  195 objective:  290.56437175978215 tr err:  0.012 gradient_magnitude:  0.01527481616848849\n","Iteration:  196 objective:  290.56437070394026 tr err:  0.012 gradient_magnitude:  0.01669704128380507\n","Iteration:  197 objective:  290.56437014978474 tr err:  0.012 gradient_magnitude:  0.012514239397511322\n","Iteration:  198 objective:  290.5643694725478 tr err:  0.012 gradient_magnitude:  0.016114718416453545\n","Iteration:  199 objective:  290.56436873724374 tr err:  0.012 gradient_magnitude:  0.012311897232461186\n","Iteration:  200 objective:  290.5643680666903 tr err:  0.012 gradient_magnitude:  0.013675783588702797\n","Iteration:  201 objective:  290.56436767581954 tr err:  0.012 gradient_magnitude:  0.011820975000082012\n","Iteration:  202 objective:  290.5643672224373 tr err:  0.012 gradient_magnitude:  0.013432290858571424\n","Iteration:  203 objective:  290.56436666309025 tr err:  0.012 gradient_magnitude:  0.010491563004901916\n","Iteration:  204 objective:  290.56436622268234 tr err:  0.012 gradient_magnitude:  0.008778191137003363\n","[ 3.38733631e-06 -5.45580842e-05 -2.24916346e-01 ... -5.13312396e-01\n"," -3.87551844e-06 -4.90900780e-02]\n","[-0.09484265]\n"]}],"source":["# Initialize the weights, step size and tolerance\n","# Start of code\n","initial_theta =  np.random.rand(feature_matrix.shape[1])## STUDENT: initialize theta\n","initial_theta0 =  np.random.rand(1) #np.ones(feature_matrix.shape[0]) ## STUDENT: initialize theta0\n","C = 1 ## STUDENT: choose the C\n","step_size = 0.25 ## STUDENT: choose the step_size\n","tolerance = 0.01 ## STUDENT: choose the tolerance\n","\n","# end of code\n","\n","theta, theta0 = adam_optimizer(train_data,train_labels, initial_theta, initial_theta0,C, step_size, tolerance)\n","print(theta)\n","print(theta0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1644726503524,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"},"user_tz":300},"id":"c0QEeEiNfT4O","outputId":"0850eb78-8326-4cc0-af13-1a94fd90b7a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training error:  0.012\n","Test error:  0.144\n"]}],"source":["# STUDENT: copy the output of this section to the solution file\n","\n","## Get predictions on training and test data\n","preds_train = model_predict(train_data,theta,theta0)\n","preds_test = model_predict(test_data,theta,theta0)\n","\n","## Compute errors\n","errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n","errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n","\n","print (\"Training error: \", float(errs_train)/len(train_labels))\n","print (\"Test error: \", float(errs_test)/len(test_labels))"]},{"cell_type":"markdown","metadata":{"id":"ta2IIHh2fT4O"},"source":["**Task P7:** List 4 example sentences that are correctly classified by SVM, and 4 example sentences that are  incorrectly classified by SVM. Explain what you have found."]},{"cell_type":"markdown","source":["**Findings and Observations** : Here we compare the test labels given to us with the lasbels that we have predicted using our SVM classisfication algorithm. Firstly we observe first four sentences that are correctly classified followed by four sentences that are misclassified.\n","Firstly we observe that most of the misclassified text are relatively longer than the correctly classified text. Thus it was diffivult for the SVM classifier to label longer sentences. It is not possible to understand distict postive or negetive sentiments from the misclassified sentences. Thus we can conclude the the texts with ambigous sentences will be difficult for classifiaction by SVM. Overall the SVM has acceptable performance."],"metadata":{"id":"0jqbQjvm1cBC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_FIPjTIfT4O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644726513510,"user_tz":300,"elapsed":543,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"}},"outputId":"571ce3f5-a103-44df-848b-f9925d140856"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentences correctly classified:\n","Label:  -1 ;  Pred label:  -1 ;  I cannot make calls at certain places.\n","Label:  -1 ;  Pred label:  -1 ;  All in all its an insult to one's intelligence and a huge waste of money.  \n","Label:  -1 ;  Pred label:  -1 ;  The plot is nonsense that doesn't interest in the slightest way or have any uniqueness to it.  \n","Label:  -1 ;  Pred label:  -1 ;  bad fit, way too big.\n"]}],"source":["score = (test_data.dot(theta)+theta0)*test_labels\n","predv = (test_data.dot(theta)+theta0)\n","inds = np.nonzero(score>1)[0]\n","print(\"Sentences correctly classified:\")\n","for i in inds[:4]:\n","    ind = np.array(test_inds)[i]\n","    print('Label: ',test_labels[i],'; ', 'Pred label: ',(predv[i]>0)*2-1,'; ',sentences[ind])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Li32SmvCfT4O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644726515761,"user_tz":300,"elapsed":860,"user":{"displayName":"Amit Dutta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiysMq2E33bnxm_OHUlC4P_cmo3c5McWFp_ZCm0=s64","userId":"01295948358215292856"}},"outputId":"69715e58-01fa-4acc-d81f-2c3e716af0b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentences incorrectly classified:\n","Label:  -1 ;  Pred label:  1 ;  Coming here is like experiencing an underwhelming relationship where both parties can't wait for the other person to ask to break up.\n","Label:  -1 ;  Pred label:  1 ;  If you do go see this movie, bring a pillow or a girlfriend/boyfriend to keep you occupied through out.  \n","Label:  -1 ;  Pred label:  1 ;  Full of unconvincing cardboard characters it is blandly written by Edward Chodorov, who also produced, and is surprisingly directed by Jean Negulesco from whom one would expect a great deal more.  \n","Label:  -1 ;  Pred label:  1 ;  You need two hands to operate the screen.This software interface is decade old and cannot compete with new software designs.\n"]}],"source":["score = (test_data.dot(theta)+theta0)*test_labels\n","predv = (test_data.dot(theta)+theta0)\n","inds = np.nonzero(score<-1)[0]\n","print(\"Sentences incorrectly classified:\")\n","for i in inds[:4]:\n","    ind = np.array(test_inds)[i]\n","    print('Label: ',test_labels[i],'; ', 'Pred label: ',(predv[i]>0)*2-1,'; ',sentences[ind])"]}],"metadata":{"anaconda-cloud":{},"colab":{"collapsed_sections":[],"name":"Programming Assignment 2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"toc":{"colors":{"hover_highlight":"#DAA520","navigate_num":"#000000","navigate_text":"#333333","running_highlight":"#FF0000","selected_highlight":"#FFD700","sidebar_border":"#EEEEEE","wrapper_background":"#FFFFFF"},"moveMenuLeft":true,"nav_menu":{"height":"12px","width":"252px"},"navigate_menu":true,"number_sections":false,"sideBar":true,"threshold":4,"toc_cell":false,"toc_section_display":"block","toc_window_display":false,"widenNotebook":false}},"nbformat":4,"nbformat_minor":0}