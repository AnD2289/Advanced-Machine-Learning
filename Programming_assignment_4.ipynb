{"cells":[{"cell_type":"markdown","metadata":{"id":"hU5au6sVA6BV"},"source":["# Distributed Representation for Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"EgXBqzDkA6BY"},"source":["In this programming assignment, we will experiment with distributed representations of words. We'll also see how such an embedding can be constructed by applying principal component analysis to a suitably transformed matrix of word co-occurrence probabilities. For computational reasons, we'll use the moderately sized **Brown corpus of present-day American English** for this."]},{"cell_type":"markdown","metadata":{"id":"nBOKdKdgA6BY"},"source":["## 1. Accessing the Brown corpus"]},{"cell_type":"markdown","metadata":{"id":"OyqZrqtTA6BY"},"source":["The *Brown corpus* is available as part of the Python Natural Language Toolkit (`nltk`)."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"tHnScUxbA6BZ","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056019239,"user_tz":240,"elapsed":2253,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"25dc6897-3f43-420a-c1ba-987eca5d1062"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import numpy as np\n","import pickle\n","import nltk\n","nltk.download('brown')\n","nltk.download('stopwords')\n","from nltk.corpus import brown, stopwords\n","from scipy.cluster.vq import kmeans2\n","from sklearn.decomposition import PCA"]},{"cell_type":"markdown","metadata":{"id":"eKIJ-6ANA6Ba"},"source":["The corpus consists of 500 samples of text drawn from a wide range of sources. When these are concatenated, they form a very long stream of over a million words, which is available as `brown.words()`. Let's look at the first 50 words."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"y-DH060CA6Ba","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056021624,"user_tz":240,"elapsed":658,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"0937442f-c5db-426a-c98a-9e97664677f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["The\n","Fulton\n","County\n","Grand\n","Jury\n","said\n","Friday\n","an\n","investigation\n","of\n","Atlanta's\n","recent\n","primary\n","election\n","produced\n","``\n","no\n","evidence\n","''\n","that\n"]}],"source":["for i in range(20):\n","    print (brown.words()[i],)"]},{"cell_type":"markdown","metadata":{"id":"DobE74xqA6Ba"},"source":["Before doing anything else, let's remove stopwords and punctuation and make everything lowercase. The resulting sequence will be stored in `my_word_stream`."]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"hbh1BQ3dA6Bb","executionInfo":{"status":"ok","timestamp":1650056026652,"user_tz":240,"elapsed":2891,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"outputs":[],"source":["my_stopwords = set(stopwords.words('english'))\n","word_stream = [str(w).lower() for w in brown.words() if w.lower() not in my_stopwords]\n","my_word_stream = [w for w in word_stream if (len(w) > 1 and w.isalnum())]"]},{"cell_type":"markdown","metadata":{"id":"ao0QClfRA6Bb"},"source":["Here are the initial 20 words in `my_word_stream`."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"t37J8YD-A6Bb","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056028041,"user_tz":240,"elapsed":4,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"d9287b54-82f7-4696-a7a8-571e6405be65"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['fulton',\n"," 'county',\n"," 'grand',\n"," 'jury',\n"," 'said',\n"," 'friday',\n"," 'investigation',\n"," 'recent',\n"," 'primary',\n"," 'election',\n"," 'produced',\n"," 'evidence',\n"," 'irregularities',\n"," 'took',\n"," 'place',\n"," 'jury',\n"," 'said',\n"," 'presentments',\n"," 'city',\n"," 'executive',\n"," 'committee',\n"," 'charge',\n"," 'election',\n"," 'deserves',\n"," 'praise']"]},"metadata":{},"execution_count":5}],"source":["my_word_stream[:25]"]},{"cell_type":"markdown","metadata":{"id":"YpZVYcy-A6Bc"},"source":["## 2. Computing co-occurrence probabilities"]},{"cell_type":"markdown","metadata":{"id":"xB7MHMk7A6Bc"},"source":["**Task P1**: Complete the following code to get a list of words and their counts. Report how many times does the word \"evidence\" and \"investigation\" appears in the corpus."]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":true,"id":"ojUlz1AVA6Bc","executionInfo":{"status":"ok","timestamp":1650056031129,"user_tz":240,"elapsed":746,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"outputs":[],"source":["N = len(my_word_stream)\n","words = []\n","totals = {}\n","\n","## STUDENT: Your code here\n","# words: a python list of unique words in the document my_word_stream as the vocabulary\n","# totals: a python dictionary, where each word is a key, and the corresponding value\n","#         is the number of times this word appears in the document my_word_stream\n","\n","#for i in range(N):\n","  #word = my_word_stream[i]\n","words = list(set(my_word_stream))\n","\n","for word in my_word_stream:\n","  #print(words)\n","  if word in totals:\n","    totals[word] += 1\n","  else:\n","    totals[word] = 1\n","## STUDENT CODE ENDS\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Yf2e-pblA6Bc","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056034209,"user_tz":240,"elapsed":521,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"4ee651f5-1b02-42fa-ab52-63d59a28f96a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word \" evidence \" appears  204  times\n","Word \" investigation \" appears  51  times\n"]}],"source":["## STUDENT: Report how many times does the word \"evidence\" and \"investigation\" appears in the corpus.\n","print('Word \"',\"evidence\",'\" appears ',totals.get(\"evidence\"), ' times')\n","print('Word \"',\"investigation\",'\" appears ',totals.get(\"investigation\"), ' times')"]},{"cell_type":"markdown","metadata":{"id":"J5O5CXJDA6Bc"},"source":["** Task P2**: Decide on the vocabulary. There are two potentially distinct vocabularies: the words for which we will obtain embeddings (`vocab_words`) and the words we will consider when looking at context information (`context_words`). We will take the former to be all words that occur at least 20 times, and the latter to be all words that occur at least 100 times. We will stick to these choices for this assignment, but feel free to play around with them and find something better.\n","\n","How large are these two word lists? Note down these numbers."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"7wPAgXdQA6Bd","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056035154,"user_tz":240,"elapsed":3,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"727f6f3d-d302-4807-f890-d62d4ca4bdc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of vocabulary words  4720 ;\n","Number of context words  918 ;\n"]}],"source":["## STUDENT: Your code here\n","\n","vocab_words =  []# a list of words whose occurances (totals) are > 19\n","context_words = [] # a list of words whose occurances (totals) are > 99\n","\n","for key, value in totals.items():\n","  if (value > 99):\n","    context_words.append(key)\n","  if (value > 19):\n","    vocab_words.append(key)\n","## STUDENT CODE ENDS\n","print('Number of vocabulary words ',len(vocab_words), ';')\n","print('Number of context words ',len(context_words), ';') "]},{"cell_type":"markdown","metadata":{"id":"ElXWOuscA6Bd"},"source":["**Task P3**: Get co-occurrence counts. These are defined as follows, for a small constant `window_size=2`.\n","\n","* Let `w0` be any word in `vocab_words` and `w` any word in `context_words`.\n","* Each time `w0` occurs in the corpus, look at the window of `window_size` words before and after it. If `w` appears in this window, we say it appears in the context of (this particular occurrence of) `w0`.\n","* Define `counts[w0][w]` as the total number of times `w` occurs in the context of `w0`.\n","\n","Complete the function `get_counts`, which computes the `counts` array and returns it as a dictionary (of dictionaries). Find how many times the word \"fact\" appears in the context of ‚Äùevidence\" with window_size=2."]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"id":"Dg-ebfYVA6Bd","executionInfo":{"status":"ok","timestamp":1650056037789,"user_tz":240,"elapsed":226,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"outputs":[],"source":["def get_counts(window_size=2):\n","    ## Input:\n","    #  window_size: for each word w0, its context includes window_size words before and after it. \n","    #  For instance, if window_size = 2, it means we look at w1 w2 w0 w3 w4, where  w1, w2, w3, w4 are \n","    #  context woreds\n","    ## Output:\n","    #  counts: a python dictionary (of dictionaries) where counts[w0][w] indicate the number of times the word w appears \n","    #  in the context of w0 (Note: counts[w0] is also a python dictionary)\n","    words_num = enumerate(my_word_stream)\n","    counts = {}\n","    for w0 in vocab_words:\n","        counts[w0] = {} \n","        \n","    ## STUDENT: Your code here\n","\n","        for index, value in enumerate(my_word_stream):\n","            if value == w0:\n","                if index < window_size:\n","                   window = my_word_stream[0:index+window_size+1]\n","                elif index+window_size > len(my_word_stream):\n","                   window = my_word_stream[index-window_size: len(my_word_stream)]\n","                else:\n","                  window = my_word_stream[index-window_size:index+window_size+1]\n","\n","                for item in context_words:\n","                    if item in window and item != value:\n","                      if item in counts[w0]:\n","                        counts[w0][item] += 1\n","                      else:\n","                        counts[w0][item] = 1\n","\n","    \n","    ## End of codes\n","    return counts"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"vkX5sPh5A6Bd","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056288932,"user_tz":240,"elapsed":248851,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"e42f132f-c341-4f50-b60b-bfec88fa81c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}],"source":["## STUDENT: Report how many times the word \"fact\" appears in the context of ‚Äùevidence\".\n","counts = get_counts(window_size=2)\n","print(counts['evidence']['fact'])"]},{"cell_type":"markdown","metadata":{"id":"Vt8PCxTaA6Be"},"source":["Define `probs[w0][]` to be the distribution over the context of `w0`, that is:\n","* `probs[w0][w] = counts[w0][w] / (sum of all counts[w0][])`\n","\n","**Task P4**: Finish the function `get_co_occurrence_dictionary` that computes `probs`. Find the probability that the word \"fact\" appears in the context of ‚Äùevidence\"."]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":true,"id":"XpzDNLH6A6Be","executionInfo":{"status":"ok","timestamp":1650056303878,"user_tz":240,"elapsed":218,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"outputs":[],"source":["def get_co_occurrence_dictionary(counts):\n","    ## Input:\n","    #  counts: a python dictionary (of dictionaries) where counts[w0][w] indicate the number of times the word w appears \n","    #  in the context of w0 (Note: counts[w0] is also a python dictionary)\n","    ## Output:\n","    #  probs: a python dictionary (of dictionaries) where probs[w0][w] indicate the probability that word w appears \n","    #  in the context of word w0\n","    \n","    probs = {}\n","    \n","    ## STUDENT: Your code here\n","\n","    for w0 in counts:\n","\n","      probs[w0] = {}\n","      for item in counts[w0]:\n","        probs[w0][item] = counts[w0][item]/sum(counts[w0].values())\n","    \n","    ## End of codes\n","    return probs"]},{"cell_type":"code","source":["sum(counts['evidence'].values())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"9mrBTHyv-tCh","executionInfo":{"status":"ok","timestamp":1650056306313,"user_tz":240,"elapsed":520,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"c082b6c1-f853-4f0a-97e8-90f790162377"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["369"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["len(my_word_stream)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"teFzl3tLHUdz","executionInfo":{"status":"ok","timestamp":1650056307458,"user_tz":240,"elapsed":2,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"64ae4165-81b2-4a66-de54-9966cbdbe1d6"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["513240"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","execution_count":14,"metadata":{"id":"LXERV6KWA6Be","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056311123,"user_tz":240,"elapsed":1210,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"d737f5f8-a1a4-494d-dd31-f058d2f5cff9"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.01084010840108401\n"]}],"source":["## STUDENT: Report how many times the word \"fact\" appears in the context of ‚Äùevidence\".\n","probs = get_co_occurrence_dictionary(counts)\n","print(probs['evidence']['fact'])"]},{"cell_type":"markdown","metadata":{"id":"KCxy-4btA6Be"},"source":["The final piece of information we need is the frequency of different context words. The function below, `get_context_word_distribution`, takes `counts` as input and returns (again, in dictionary form) the array:\n","\n","* `context_frequency[w]` = sum of all `counts[][w]` / sum of all `counts[][]` "]},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":true,"id":"C-wRtOw5A6Be","executionInfo":{"status":"ok","timestamp":1650056312647,"user_tz":240,"elapsed":220,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"outputs":[],"source":["def get_context_word_distribution(counts):\n","    counts_context = {}\n","    sum_context = 0\n","    context_frequency = {}\n","    for w in context_words:\n","        counts_context[w] = 0\n","    for w0 in counts.keys():\n","        for w in counts[w0].keys():\n","            counts_context[w] = counts_context[w] + counts[w0][w]\n","            sum_context = sum_context + counts[w0][w]\n","    for w in context_words:\n","        context_frequency[w] = float(counts_context[w])/float(sum_context)\n","    return context_frequency"]},{"cell_type":"markdown","metadata":{"id":"8DnCqcM2A6Be"},"source":["## 3. The embedding"]},{"cell_type":"markdown","metadata":{"id":"SazBldQzA6Bf"},"source":["**Task P5**: Based on the various pieces of information above, we compute the **pointwise mutual information matrix**:\n","* `PMI[i,j] = MAX(0, log probs[ith vocab word][jth context word] - log context_frequency[jth context word])`\n","\n","Complete the code to compute PMI for every word i and context word j. Report the output of the code."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"SfIs7-nsA6Bf","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056325851,"user_tz":240,"elapsed":6295,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"6e132fcb-1191-44b3-a96f-561e9a0dba2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computing counts and distributions\n","Computing pointwise mutual information\n"]}],"source":["print (\"Computing counts and distributions\")\n","#counts = get_counts(2)\n","probs = get_co_occurrence_dictionary(counts)\n","context_frequency = get_context_word_distribution(counts)\n","#\n","print (\"Computing pointwise mutual information\")\n","n_vocab = len(vocab_words)\n","n_context = len(context_words)\n","pmi = np.zeros((n_vocab, n_context))\n","for i in range(0, n_vocab):\n","    w0 = vocab_words[i]\n","    for w in probs[w0].keys():\n","        j = context_words.index(w)\n","        ## STUDENT: Your code here\n","        pmi[i,j] = max(0,np.log(probs[w0][context_words[j]]) - np.log(context_frequency[context_words[j]])) \n","        ## Student end of code"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"xwAYHF_rA6Bf","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650056325851,"user_tz":240,"elapsed":6,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"d76e68a7-e98d-40ab-ab87-816b7340ffc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.6848183447904903\n"]}],"source":["# STUDENT: report the following number\n","print(pmi[vocab_words.index('evidence'),context_words.index('fact')])"]},{"cell_type":"markdown","metadata":{"id":"z8JvnHr3A6Bf"},"source":["The embedding of any word can then be taken as the corresponding row of this matrix. However, to reduce the dimension, we will apply principal component analysis (PCA).\n","\n","See this nice tutorial on PCA: https://www.youtube.com/watch?v=fkf4IBRSeEc\n","\n","Now reduce the dimension of the PMI vectors using principal component analysis. Here we bring it down to 100 dimensions, and then normalize the vectors to unit length."]},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":true,"id":"wvYxH3QlA6Bf","executionInfo":{"status":"ok","timestamp":1650056329774,"user_tz":240,"elapsed":1193,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"outputs":[],"source":["pca = PCA(n_components=100)\n","vecs = pca.fit_transform(pmi)\n","for i in range(0,n_vocab):\n","    vecs[i] = vecs[i]/np.linalg.norm(vecs[i])"]},{"cell_type":"code","source":["print(vecs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"MIP2TJolSpia","executionInfo":{"status":"ok","timestamp":1650056331316,"user_tz":240,"elapsed":2,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"d47366c1-db51-4df5-b590-2bae29beea8e"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["(4720, 100)\n"]}]},{"cell_type":"markdown","metadata":{"id":"oPekGg15A6Bf"},"source":["It is useful to save this embedding so that it doesn't need to be computed every time."]},{"cell_type":"code","execution_count":20,"metadata":{"collapsed":true,"id":"8vIt1o-MA6Bf","executionInfo":{"status":"ok","timestamp":1650056334141,"user_tz":240,"elapsed":185,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"outputs":[],"source":["fd = open(\"embedding.pickle\", \"wb\")\n","pickle.dump(vocab_words, fd)\n","pickle.dump(context_words, fd)\n","pickle.dump(vecs, fd)\n","fd.close()"]},{"cell_type":"markdown","metadata":{"id":"I7UIjGjtA6Bf"},"source":["## 4. Experimenting with the embedding"]},{"cell_type":"markdown","metadata":{"id":"bSorwUTzA6Bf"},"source":["We can get some insight into the embedding by looking at some intersting use cases.\n","\n","** Task P6**: Implement the following function that finds the nearest neighbor of a given word in the embedded space. Note down the answers to the following queries. "]},{"cell_type":"code","execution_count":21,"metadata":{"collapsed":true,"id":"LcG1tbVgA6Bf","executionInfo":{"status":"ok","timestamp":1650056338533,"user_tz":240,"elapsed":541,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"outputs":[],"source":["def word_NN(w,vecs,vocab_words,context_words):\n","    ## Input:\n","    #  w: word w\n","    #  vecs: the embedding of words, as computed above\n","    #  vocab_words: vocabulary words, as computed in Task P2\n","    #  context_words: context words, as computed in Task P2\n","    ## Output:\n","    #  the nearest neighbor (word) to word w\n","    if not(w in vocab_words):\n","        print (\"Unknown word\")\n","        return\n","    \n","    ## Student: your code here\n","    v = vecs[vocab_words.index(w)]\n","    neighbour = 0\n","    curr_dist = np.linalg.norm(v-vecs[0])\n","    for i in range(1,n_vocab):\n","      dist = np.linalg.norm(v-vecs[i])\n","      if (dist<curr_dist) and (dist>0.0):\n","        neighbour = i\n","        curr_dist = dist\n","    \n","    return vocab_words[neighbour]\n","    ## Student: code ends"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33cBQiIrA6Bg","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1649981571971,"user_tz":240,"elapsed":89,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"bcde71a5-1d52-4e4d-a82c-babe3fd110ff"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'nations'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":121}],"source":["word_NN('world',vecs,vocab_words,context_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-n4e2lwaA6Bg","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1649981642032,"user_tz":240,"elapsed":92,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"ada3ba4e-43fa-4775-97fc-06700e4b3d70"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'skill'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":149}],"source":["word_NN('learning',vecs,vocab_words,context_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFVs7B6jA6Bg","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1649981604859,"user_tz":240,"elapsed":90,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"f36f3d6c-a01a-4d39-a3a0-70276c15f1cc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'studies'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":134}],"source":["word_NN('technology',vecs,vocab_words,context_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pY7AMczeA6Bg","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1649981559849,"user_tz":240,"elapsed":99,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"7962459c-cbe8-4385-e66c-fdc1ecede35d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'woman'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":117}],"source":["word_NN('man',vecs,vocab_words,context_words)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ReJJrfj_A6Bg"},"source":["** Task P7**: Implement the function that aims to solve the analogy problem:\n","A is to B as C is to ?\n","For example, A=King, B=Queen, C=man, and the answer for ? should be ideally woman (you will see that this may not be  the case using the distributed representation).\n","\n","Finds the K-nearest neighbor of a given word in the embedded space. Note: instead of outputing only the nearest neighbor, you should find the K=10 nearest neighbors and see whether there is one in the list that makes sense. You should also exclude the words C in the output list.\n","\n","Also report another set A, B, C and the corresponding answer output by your problem. See if it makes sense to you."]},{"cell_type":"code","source":["def find_analogy(A,B,C,vecs,vocab_words,context_words):\n","    ## Input:\n","    #  A, B, C: words A, B, C\n","    #  vecs: the embedding of words, as computed above\n","    #  vocab_words: vocabulary words, as computed in Task P2\n","    #  context_words: context words, as computed in Task P2\n","    ## Output:\n","    #  the word that solves the analogy problem\n","    ## STUDENT: Your code here\n","    v = vecs[vocab_words.index(C)] - (vecs[vocab_words.index(A)]-vecs[vocab_words.index(B)])\n","    neighbor = []\n","    curr_dist = np.linalg.norm(v - vecs[0])\n","    #neighbor_stack.append((0,curr_dist))\n","\n","    for i in range(1,n_vocab):\n","      if (vocab_words[i]!=C):\n","        dist = np.linalg.norm(v-vecs[i])\n","        neighbor.append((dist,i))\n","        if (dist < curr_dist) and (dist>0.0):\n","          \n","          curr_dist = dist\n","\n","    \n","    neighbor = sorted(neighbor)\n","    neighbor = neighbor[:10]\n","    #print(neighbor)\n","  \n","\n","    nearest_words = []\n","\n","    for item in neighbor:\n","      #print(vocab_words[item[1]])\n","      nearest_words.append(vocab_words[item[1]])\n","\n","    return nearest_words\n","    ## STUDENT: your code ends"],"metadata":{"id":"r3ZkjgO6GvY6","executionInfo":{"status":"ok","timestamp":1650060504119,"user_tz":240,"elapsed":205,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}}},"execution_count":210,"outputs":[]},{"cell_type":"code","source":["vocab_words[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"rMh92LYJH9Sc","executionInfo":{"status":"ok","timestamp":1650058965675,"user_tz":240,"elapsed":5,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"75b1d3df-1331-4641-d427-0be5fff5f771"},"execution_count":154,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'county'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":154}]},{"cell_type":"code","execution_count":211,"metadata":{"id":"awslASgIA6Bg","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650060506317,"user_tz":240,"elapsed":408,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"e020584f-726c-473e-8dc5-f3cddd263a89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['queen',\n"," 'flesh',\n"," 'soul',\n"," 'fed',\n"," 'sad',\n"," 'dead',\n"," 'woman',\n"," 'telling',\n"," 'kate',\n"," 'brave']"]},"metadata":{},"execution_count":211}],"source":["find_analogy('king','queen','man',vecs,vocab_words,context_words)"]},{"cell_type":"code","execution_count":212,"metadata":{"id":"GCB_SViGA6Bg","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1650060508269,"user_tz":240,"elapsed":185,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"8298b8e6-2ac1-4b83-be47-748f5bf005d3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['grass',\n"," 'shoulders',\n"," 'rain',\n"," 'suddenly',\n"," 'grabbed',\n"," 'blue',\n"," 'beneath',\n"," 'hung',\n"," 'stood',\n"," 'threw']"]},"metadata":{},"execution_count":212}],"source":["find_analogy('soil','grass','sun',vecs,vocab_words,context_words) "]},{"cell_type":"code","source":["find_analogy('wife','married','love',vecs,vocab_words,context_words) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"DkXZhJJjQOXt","executionInfo":{"status":"ok","timestamp":1650060511058,"user_tz":240,"elapsed":448,"user":{"displayName":"Amit Dutta","userId":"01295948358215292856"}},"outputId":"13e69caa-aee4-46c3-dc3b-7b5ad55c769c"},"execution_count":213,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['married',\n"," 'knows',\n"," 'live',\n"," 'god',\n"," 'wondered',\n"," 'knew',\n"," 'faith',\n"," 'spirit',\n"," 'christ',\n"," 'lost']"]},"metadata":{},"execution_count":213}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"toc":{"colors":{"hover_highlight":"#DAA520","navigate_num":"#000000","navigate_text":"#333333","running_highlight":"#FF0000","selected_highlight":"#FFD700","sidebar_border":"#EEEEEE","wrapper_background":"#FFFFFF"},"moveMenuLeft":true,"nav_menu":{"height":"12px","width":"252px"},"navigate_menu":true,"number_sections":false,"sideBar":true,"threshold":4,"toc_cell":false,"toc_section_display":"block","toc_window_display":false,"widenNotebook":false},"colab":{"name":"Programming_assignment_4.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}